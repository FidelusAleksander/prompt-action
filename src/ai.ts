import { OpenAI } from 'openai'

export async function generateAIResponse(
  prompt: string,
  systemPrompt: string,
  model: string,
  token: string,
  schema?: { [key: string]: unknown }
): Promise<string> {
  const client = new OpenAI({
    baseURL: 'https://models.inference.ai.azure.com',
    apiKey: token
  })

  try {
    const completionOptions: {
      model: string
      messages: Array<{ role: 'system' | 'user'; content: string }>
      response_format?: {
        type: 'json_schema'
        json_schema: {
          name: string
          schema: { [key: string]: unknown }
          strict: boolean
        }
      }
    } = {
      model: model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: prompt }
      ]
    }

    // Add structured output format if schema is provided
    if (schema) {
      completionOptions.response_format = {
        type: 'json_schema',
        json_schema: {
          name: 'structured_response',
          schema: schema,
          strict: false
        }
      }
    }

    const completion = await client.chat.completions.create(completionOptions)

    const response = completion.choices[0]?.message?.content
    if (!response) {
      throw new Error('No response was generated by the AI model')
    }

    return response
  } catch (error) {
    throw new Error(
      `Failed to generate AI response: ${
        error instanceof Error ? error.message : String(error)
      }`
    )
  }
}
